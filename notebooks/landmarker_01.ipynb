{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holo_table.utils.cv import cv_imshow\n",
    "from holo_table.utils.mediapipe import get_default_hand_connections\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import mediapipe.python.solutions.drawing_styles as mp_drawing_styles\n",
    "import mediapipe.python.solutions.drawing_utils as mp_drawing_utils\n",
    "import mediapipe.python.solutions.hands as mp_hands\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 3\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # vibrant green\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend(\n",
    "            [\n",
    "                landmark_pb2.NormalizedLandmark(\n",
    "                    x=landmark.x, y=landmark.y, z=landmark.z\n",
    "                )\n",
    "                for landmark in hand_landmarks\n",
    "            ]\n",
    "        )\n",
    "        mp_drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            get_default_hand_connections(),\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style(),\n",
    "        )\n",
    "\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        cv.putText(\n",
    "            annotated_image,\n",
    "            f\"{handedness[0].category_name}\",\n",
    "            (text_x, text_y),\n",
    "            cv.FONT_HERSHEY_DUPLEX,\n",
    "            FONT_SIZE,\n",
    "            HANDEDNESS_TEXT_COLOR,\n",
    "            FONT_THICKNESS,\n",
    "            cv.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return annotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_model_fol = Path(\"~/.mediapipe/models\").expanduser()\n",
    "hand_landmark_model_path = mp_model_fol / \"hand_landmarker.task\"\n",
    "if not hand_landmark_model_path.exists():\n",
    "    print(\n",
    "        \"Download the hand landmark model using\\n\"\n",
    "        \"curl \"\n",
    "        \"https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/hand_landmarker.task\"\n",
    "        f\" --create-dirs -o {hand_landmark_model_path}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fol = Path(\"../data/sample\").absolute().resolve()\n",
    "sample_image_path = data_fol / \"woman_hands.jpg\"\n",
    "if not sample_image_path.exists():\n",
    "    print(\n",
    "        \"Download the sample image using\\n\"\n",
    "        \"curl \"\n",
    "        \"https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg\"\n",
    "        f\" --create-dirs -o {sample_image_path}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.core.base_options import BaseOptions\n",
    "from mediapipe.tasks.python.vision.hand_landmarker import HandLandmarkerOptions\n",
    "from mediapipe.tasks.python.vision.hand_landmarker import HandLandmarker\n",
    "\n",
    "# from here\n",
    "# https://github.com/google/mediapipe/blob/master/mediapipe/python/pybind/image.cc\n",
    "# from mediapipe.python._framework_bindings\n",
    "# ._framework_bindings.image import Image\n",
    "\n",
    "# STEP 2: Create an ImageClassifier object.\n",
    "base_options = BaseOptions(model_asset_path=str(hand_landmark_model_path))\n",
    "options = HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "detector = HandLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(str(sample_image_path))\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "cv_imshow(cv.cvtColor(annotated_image, cv.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # or import it like this but still no type hints\n",
    "# from mediapipe.python import Image\n",
    "# Image.create_from_file(str(sample_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    image.width,\n",
    "    image.height,\n",
    "    image.channels,\n",
    "    image.numpy_view().shape,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo-table-gwMx93Vl-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
